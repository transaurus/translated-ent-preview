---
title: "Building RAG systems in Go with Ent, Atlas, and pgvector"
author: Rotem Tamir
authorURL: "https://github.com/rotemtam"
authorImageURL: "https://s.gravatar.com/avatar/36b3739951a27d2e37251867b7d44b1a?s=80"
authorTwitter: _rtam
image: "https://atlasgo.io/uploads/entrag.png"
---

本篇博文将探讨如何利用[Ent](https://entgo.io)、[Atlas](https://atlasgo.io)和[pgvector](https://github.com/pgvector/pgvector)构建一个[RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)（检索增强生成）系统。

RAG是一种通过引入检索步骤来增强生成模型能力的技术。不同于仅依赖模型内部知识，该方法能从外部数据源检索相关文档或数据，从而生成更准确、更具上下文感知的响应。这一技术特别适用于构建问答系统、聊天机器人等需要最新或领域特定知识的应用场景。

### 配置Ent数据模型

首先初始化项目所需的Go模块：

```bash
go mod init github.com/rotemtam/entrag # Feel free to replace the module path with your own
```

本项目将使用Go的实体框架[Ent](/)定义数据库模型。数据库将存储待检索文档（按固定大小分块）及每个文本块的向量表示。运行以下命令初始化Ent项目：

```bash
go run -mod=mod entgo.io/ent/cmd/ent new Embedding Chunk
```

该命令会创建数据模型的占位文件，项目结构如下：

```
├── ent
│   ├── generate.go
│   └── schema
│       ├── chunk.go
│       └── embedding.go
├── go.mod
└── go.sum
```

接着定义`Chunk`模型的结构。打开`ent/schema/chunk.go`文件进行如下定义：

```go title="ent/schema/chunk.go"
package schema

import (
	"entgo.io/ent"
	"entgo.io/ent/schema/edge"
	"entgo.io/ent/schema/field"
)

// Chunk holds the schema definition for the Chunk entity.
type Chunk struct {
	ent.Schema
}

// Fields of the Chunk.
func (Chunk) Fields() []ent.Field {
	return []ent.Field{
		field.String("path"),
		field.Int("nchunk"),
		field.Text("data"),
	}
}

// Edges of the Chunk.
func (Chunk) Edges() []ent.Edge {
	return []ent.Edge{
		edge.To("embedding", Embedding.Type).StorageKey(edge.Column("chunk_id")).Unique(),
	}
}
```

该模型定义了包含三个字段的`Chunk`实体：`path`存储文档路径，`nchunk`存储分块序号，`data`存储分块文本数据。同时定义了指向`Embedding`实体的边关系，用于存储文本块的向量表示。

继续之前，需安装`pgvector`扩展包。该PostgreSQL扩展提供向量运算和相似性搜索支持，用于存储和检索文本块的向量表示。

```bash
go get github.com/pgvector/pgvector-go
```

接下来定义`Embedding`模型。打开`ent/schema/embedding.go`文件进行如下配置：

```go title="ent/schema/embedding.go"
package schema

import (
	"entgo.io/ent"
	"entgo.io/ent/dialect"
	"entgo.io/ent/dialect/entsql"
	"entgo.io/ent/schema/edge"
	"entgo.io/ent/schema/field"
	"entgo.io/ent/schema/index"
	"github.com/pgvector/pgvector-go"
)

// Embedding holds the schema definition for the Embedding entity.
type Embedding struct {
	ent.Schema
}

// Fields of the Embedding.
func (Embedding) Fields() []ent.Field {
	return []ent.Field{
		field.Other("embedding", pgvector.Vector{}).
			SchemaType(map[string]string{
				dialect.Postgres: "vector(1536)",
			}),
	}
}

// Edges of the Embedding.
func (Embedding) Edges() []ent.Edge {
	return []ent.Edge{
		edge.From("chunk", Chunk.Type).Ref("embedding").Unique().Required(),
	}
}

func (Embedding) Indexes() []ent.Index {
	return []ent.Index{
		index.Fields("embedding").
			Annotations(
				entsql.IndexType("hnsw"),
				entsql.OpClass("vector_l2_ops"),
			),
	}
}
```

该模型定义了包含`pgvector.Vector`类型字段`embedding`的实体，用于存储文本块向量。通过定义指向`Chunk`实体的边关系，并在`embedding`字段上使用`hnsw`索引类型和`vector_l2_ops`操作符类建立索引，可实现高效的向量相似性搜索。

最后运行以下命令生成Ent代码：

```bash
go mod tidy
go generate ./...
```

Ent将根据模型定义自动生成所需代码。

### 数据库配置

接下来配置PostgreSQL数据库。我们将使用Docker运行本地PostgreSQL实例，由于需要`pgvector`扩展，选择预装该扩展的`pgvector/pgvector:pg17`镜像。

```bash
docker run --rm --name postgres -e POSTGRES_PASSWORD=pass -p 5432:5432 -d pgvector/pgvector:pg17
```

我们将使用与Ent集成的数据库Schema-as-Code工具[Atlas](https://atlasgo.io)来管理数据库模型。运行以下命令安装Atlas：

```
curl -sSfL https://atlasgo.io/install.sh | sh
```

其他安装方式请参阅[Atlas安装文档](https://atlasgo.io/getting-started#installation)。

由于需要管理扩展功能，您需注册Atlas Pro账户。可通过以下命令申请免费试用：

```
atlas login
```

:::note[不使用迁移工具的情况]

若希望跳过Atlas的使用，您可以直接通过[此文件](https://github.com/rotemtam/entrag/blob/e91722c0fbe011b03dbd6b9e68415547c8b7bba4/setup.sql#L1)中的SQL语句将所需模式直接应用到数据库。

:::

现在，让我们创建Atlas配置文件，将`base.pg.hcl`文件与Ent模式组合起来：

```hcl title="atlas.hcl"
data "composite_schema" "schema" {
  schema {
    url = "file://base.pg.hcl"
  }
  schema "public" {
    url = "ent://ent/schema"
  }
}

env "local" {
  url = getenv("DB_URL")
  schema {
    src = data.composite_schema.schema.url
  }
  dev = "docker://pgvector/pg17/dev"
}
```

该配置定义了一个复合模式，包含`base.pg.hcl`文件和Ent模式。我们还定义了一个名为`local`的环境，使用该复合模式进行本地开发。`dev`字段指定了[开发数据库](https://atlasgo.io/concepts/dev-database)的URL，Atlas利用该URL对模式进行规范化处理并执行各类计算。

接下来，运行以下命令将模式应用到数据库：

```bash
export DB_URL='postgresql://postgres:pass@localhost:5432/postgres?sslmode=disable'
atlas schema apply --env local
```

Atlas会从配置中加载数据库的目标状态，与数据库当前状态进行对比，并创建迁移计划以使数据库达到目标状态：

```
Planning migration statements (5 in total):

  -- create extension "vector":
    -> CREATE EXTENSION "vector" WITH SCHEMA "public" VERSION "0.8.0";
  -- create "chunks" table:
    -> CREATE TABLE "public"."chunks" (
         "id" bigint NOT NULL GENERATED BY DEFAULT AS IDENTITY,
         "path" character varying NOT NULL,
         "nchunk" bigint NOT NULL,
         "data" text NOT NULL,
         PRIMARY KEY ("id")
       );
  -- create "embeddings" table:
    -> CREATE TABLE "public"."embeddings" (
         "id" bigint NOT NULL GENERATED BY DEFAULT AS IDENTITY,
         "embedding" public.vector(1536) NOT NULL,
         "chunk_id" bigint NOT NULL,
         PRIMARY KEY ("id"),
         CONSTRAINT "embeddings_chunks_embedding" FOREIGN KEY ("chunk_id") REFERENCES "public"."chunks" ("id") ON UPDATE NO ACTION ON DELETE NO ACTION
       );
  -- create index "embedding_embedding" to table: "embeddings":
    -> CREATE INDEX "embedding_embedding" ON "public"."embeddings" USING hnsw ("embedding" vector_l2_ops);
  -- create index "embeddings_chunk_id_key" to table: "embeddings":
    -> CREATE UNIQUE INDEX "embeddings_chunk_id_key" ON "public"."embeddings" ("chunk_id");

-------------------------------------------

Analyzing planned statements (5 in total):

  -- non-optimal columns alignment:
    -- L4: Table "chunks" has 8 redundant bytes of padding per row. To reduce disk space,
       the optimal order of the columns is as follows: "id", "nchunk", "path",
       "data" https://atlasgo.io/lint/analyzers#PG110
  -- ok (370.25µs)

  -------------------------
  -- 114.306667ms
  -- 5 schema changes
  -- 1 diagnostic

-------------------------------------------

? Approve or abort the plan:
  ▸ Approve and apply
    Abort
```

除了规划变更外，Atlas还会提供诊断信息及模式优化建议。本例中它建议调整`chunks`表的列顺序以减少磁盘空间占用。由于本教程不关注磁盘空间问题，我们可以选择`Approve and apply`继续执行迁移。

最后，为验证模式是否成功应用，可重新运行`atlas schema apply`命令。Atlas将输出：

```bash
Schema is synced, no changes to be made
```

### 搭建CLI框架

数据库模式设置完成后，让我们搭建CLI应用程序框架。本教程将使用[`alecthomas/kong`](https://github.com/alecthomas/kong)库构建一个小型应用，用于加载、索引和查询数据库中的文档。

首先安装`kong`库：

```bash
go get github.com/alecthomas/kong
```

接着创建名为`cmd/entrag/main.go`的新文件，按如下方式定义CLI应用：

```go title="cmd/entrag/main.go"
package main

import (
	"fmt"
	"os"

	"github.com/alecthomas/kong"
)

// CLI holds global options and subcommands.
type CLI struct {
	// DBURL is read from the environment variable DB_URL.
	DBURL     string `kong:"env='DB_URL',help='Database URL for the application.'"`
	OpenAIKey string `kong:"env='OPENAI_KEY',help='OpenAI API key for the application.'"`

	// Subcommands
	Load  *LoadCmd  `kong:"cmd,help='Load command that accepts a path.'"`
	Index *IndexCmd `kong:"cmd,help='Create embeddings for any chunks that do not have one.'"`
	Ask   *AskCmd   `kong:"cmd,help='Ask a question about the indexed documents'"`
}

func main() {
	var cli CLI
	app := kong.Parse(&cli,
		kong.Name("entrag"),
		kong.Description("Ask questions about markdown files."),
		kong.UsageOnError(),
	)
	if err := app.Run(&cli); err != nil {
		fmt.Fprintf(os.Stderr, "Error: %s\n", err)
		os.Exit(1)
	}
}
```

再创建名为`cmd/entrag/rag.go`的文件，内容如下：

```go title="cmd/entrag/rag.go"
package main

type (
	// LoadCmd loads the markdown files into the database.
	LoadCmd struct {
		Path string `help:"path to dir with markdown files" type:"existingdir" required:""`
	}
	// IndexCmd creates the embedding index on the database.
	IndexCmd struct {
	}
	// AskCmd is another leaf command.
	AskCmd struct {
		// Text is the positional argument for the ask command.
		Text string `kong:"arg,required,help='Text for the ask command.'"`
	}
)
```

运行以下命令验证搭建的CLI应用是否正常工作：

```bash
go run ./cmd/entrag --help
```

若一切配置正确，您将看到CLI应用的帮助输出：

```
Usage: entrag <command> [flags]

Ask questions about markdown files.

Flags:
  -h, --help                  Show context-sensitive help.
      --dburl=STRING          Database URL for the application ($DB_URL).
      --open-ai-key=STRING    OpenAI API key for the application ($OPENAI_KEY).

Commands:
  load --path=STRING [flags]
    Load command that accepts a path.

  index [flags]
    Create embeddings for any chunks that do not have one.

  ask <text> [flags]
    Ask a question about the indexed documents

Run "entrag <command> --help" for more information on a command.
```

### 将文档加载至数据库

接下来需要准备要加载到数据库的markdown文件。创建名为`data`的目录并添加若干markdown文件。本示例中，我下载了[`ent/ent`](https://github.com/ent/ent)仓库并使用其`docs`目录作为markdown文件源。

现在实现`LoadCmd`命令以将markdown文件加载到数据库。打开`cmd/entrag/rag.go`文件并添加以下代码：

```go title="cmd/entrag/rag.go"
const (
	tokenEncoding = "cl100k_base"
	chunkSize     = 1000
)

// Run is the method called when the "load" command is executed.
func (cmd *LoadCmd) Run(ctx *CLI) error {
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	tokTotal := 0
	return filepath.WalkDir(ctx.Load.Path, func(path string, d fs.DirEntry, err error) error {
		if filepath.Ext(path) == ".mdx" || filepath.Ext(path) == ".md" {
			chunks := breakToChunks(path)
			for i, chunk := range chunks {
				tokTotal += len(chunk)
				client.Chunk.Create().
					SetData(chunk).
					SetPath(path).
					SetNchunk(i).
					SaveX(context.Background())
			}
		}
		return nil
	})
}

func (c *CLI) entClient() (*ent.Client, error) {
	return ent.Open("postgres", c.DBURL)
}
```

这段代码定义了`LoadCmd`命令的`Run`方法。该方法从指定路径读取markdown文件，将其拆分为每1000个token的块，并保存到数据库。我们使用`entClient`方法通过CLI选项中指定的数据库URL创建新的Ent客户端。

关于`breakToChunks`的具体实现，请参考[完整代码](https://github.com/rotemtam/entrag/blob/93291e0c8479ecabd5f2a2e49fbaa8c49f995e70/cmd/entrag/rag.go#L157)
（位于[`entrag`代码库](https://github.com/rotemtam/entrag)），该实现基本基于
[Eli Bendersky关于Go语言RAG的入门指南](https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/)。

最后，运行`load`命令将Markdown文件加载至数据库：

```bash
go run ./cmd/entrag load --path=data
```

命令执行完成后，数据块应已存入数据库。可通过以下命令验证：

```bash
docker exec -it postgres psql -U postgres -d postgres -c "SELECT COUNT(*) FROM chunks;"
```

预期输出类似：

```
  count
-------
   276
(1 row)
```

### 生成嵌入向量索引

文档加载完成后，我们需要为每个文本块创建嵌入向量。这里使用OpenAI API生成嵌入向量，首先需安装`openai`包：

```bash
go get github.com/sashabaranov/go-openai
```

若未持有OpenAI API密钥，可前往[OpenAI平台](https://platform.openai.com/signup)注册账号并[生成API密钥](https://platform.openai.com/api-keys)。

我们将通过环境变量`OPENAI_KEY`读取该密钥，请先进行设置：

```bash
export OPENAI_KEY=<your OpenAI API key>
```

接下来实现`IndexCmd`命令以生成文本块嵌入向量。编辑`cmd/entrag/rag.go`文件并添加以下代码：

```go title="cmd/entrag/rag.go"
// Run is the method called when the "index" command is executed.
func (cmd *IndexCmd) Run(cli *CLI) error {
	client, err := cli.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	ctx := context.Background()
	chunks := client.Chunk.Query().
		Where(
			chunk.Not(
				chunk.HasEmbedding(),
			),
		).
		Order(ent.Asc(chunk.FieldID)).
		AllX(ctx)
	for _, ch := range chunks {
		log.Println("Created embedding for chunk", ch.Path, ch.Nchunk)
		embedding := getEmbedding(ch.Data)
		_, err := client.Embedding.Create().
			SetEmbedding(pgvector.NewVector(embedding)).
			SetChunk(ch).
			Save(ctx)
		if err != nil {
			return fmt.Errorf("error creating embedding: %v", err)
		}
	}
	return nil
}

// getEmbedding invokes the OpenAI embedding API to calculate the embedding
// for the given string. It returns the embedding.
func getEmbedding(data string) []float32 {
	client := openai.NewClient(os.Getenv("OPENAI_KEY"))
	queryReq := openai.EmbeddingRequest{
		Input: []string{data},
		Model: openai.AdaEmbeddingV2,
	}
	queryResponse, err := client.CreateEmbeddings(context.Background(), queryReq)
	if err != nil {
		log.Fatalf("Error getting embedding: %v", err)
	}
	return queryResponse.Data[0].Embedding
}
```

这里定义了`IndexCmd`命令的`Run`方法：查询未生成嵌入向量的文本块，通过OpenAI API创建嵌入向量后存入数据库。

最后执行`index`命令生成嵌入向量：

```bash
go run ./cmd/entrag index
```

预期看到类似日志输出：

```
2025/02/13 13:04:42 Created embedding for chunk /Users/home/entr/data/md/aggregate.md 0
2025/02/13 13:04:43 Created embedding for chunk /Users/home/entr/data/md/ci.mdx 0
2025/02/13 13:04:44 Created embedding for chunk /Users/home/entr/data/md/ci.mdx 1
2025/02/13 13:04:45 Created embedding for chunk /Users/home/entr/data/md/ci.mdx 2
2025/02/13 13:04:46 Created embedding for chunk /Users/home/entr/data/md/code-gen.md 0
2025/02/13 13:04:47 Created embedding for chunk /Users/home/entr/data/md/code-gen.md 1
```

### 提问功能

完成文档加载和嵌入向量生成后，可实现`AskCmd`命令来查询已索引文档。编辑`cmd/entrag/rag.go`文件并添加：

```go title="cmd/entrag/rag.go"
// Run is the method called when the "ask" command is executed.
func (cmd *AskCmd) Run(ctx *CLI) error {
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	question := cmd.Text
	emb := getEmbedding(question)
	embVec := pgvector.NewVector(emb)
	embs := client.Embedding.
		Query().
		Order(func(s *sql.Selector) {
			s.OrderExpr(sql.ExprP("embedding <-> $1", embVec))
		}).
		WithChunk().
		Limit(5).
		AllX(context.Background())
	b := strings.Builder{}
	for _, e := range embs {
		chnk := e.Edges.Chunk
		b.WriteString(fmt.Sprintf("From file: %v\n", chnk.Path))
		b.WriteString(chnk.Data)
	}
	query := fmt.Sprintf(`Use the below information from the ent docs to answer the subsequent question.
Information:
%v

Question: %v`, b.String(), question)
	oac := openai.NewClient(ctx.OpenAIKey)
	resp, err := oac.CreateChatCompletion(
		context.Background(),
		openai.ChatCompletionRequest{
			Model: openai.GPT4o,
			Messages: []openai.ChatCompletionMessage{

				{
					Role:    openai.ChatMessageRoleUser,
					Content: query,
				},
			},
		},
	)
	if err != nil {
		return fmt.Errorf("error creating chat completion: %v", err)
	}
	choice := resp.Choices[0]
	out, err := glamour.Render(choice.Message.Content, "dark")
	fmt.Print(out)
	return nil
}
```

至此所有组件已完成集成。在数据库准备好文档及其嵌入向量后，现在可以对其进行提问。`AskCmd`命令的工作流程如下：

```go
emb := getEmbedding(question)
embVec := pgvector.NewVector(emb)
embs := client.Embedding.
    Query().
    Order(func(s *sql.Selector) {
        s.OrderExpr(sql.ExprP("embedding <-> $1", embVec))
    }).
    WithChunk().
    Limit(5).
    AllX(context.Background())
```

首先通过OpenAI API将用户问题转换为向量。利用该向量在数据库中查找最相似的嵌入向量，使用pgvector的`<->`运算符按相似度排序，并限制返回前5条结果。

```go
for _, e := range embs {
		chnk := e.Edges.Chunk
		b.WriteString(fmt.Sprintf("From file: %v\n", chnk.Path))
		b.WriteString(chnk.Data)
	}
	query := fmt.Sprintf(`Use the below information from the ent docs to answer the subsequent question.
Information:
%v

Question: %v`, b.String(), question)
```

随后将前5个文本块的信息组合为问题上下文，并将问题和上下文格式化为单一字符串。

```go
oac := openai.NewClient(ctx.OpenAIKey)
resp, err := oac.CreateChatCompletion(
    context.Background(),
    openai.ChatCompletionRequest{
        Model: openai.GPT4o,
        Messages: []openai.ChatCompletionMessage{

            {
                Role:    openai.ChatMessageRoleUser,
                Content: query,
            },
        },
    },
)
if err != nil {
    return fmt.Errorf("error creating chat completion: %v", err)
}
choice := resp.Choices[0]
out, err := glamour.Render(choice.Message.Content, "dark")
fmt.Print(out)
```

最后通过OpenAI API生成问题回答。我们将问题和上下文传递给API获取响应，并使用`glamour`包在终端中渲染输出。

执行`ask`命令前，请先安装`glamour`包：

```bash
go get github.com/charmbracelet/glamour
```

最后，运行 `ask` 命令来对已索引的文档提出问题：

```bash
go run ./cmd/entrag ask "tl;dr What is Ent?"
```

我们的RAG系统会给出如下响应：

```
  Ent is an open-source entity framework (ORM) for the Go programming language. It
  allows developers to define data models or graph-structures in Go code. Ent
  emphasizes principles such as schema as code, a statically typed and explicit
  API generated through codegen, simple queries and graph traversals, statically
  typed predicates, and storage agnosticism. It supports various databases,
  including MySQL, MariaDB, PostgreSQL, SQLite, and Gremlin-based graph databases,
  and aims to enhance productivity in Go development.
```

太棒了！我们已成功使用Ent、Atlas和pgvector构建了一个RAG系统。现在可以向数据库中加载的文档提问，并获得结合上下文的智能回答。

以下是这个精巧的RAG系统更多的问答示例：

<details>
<summary>go run ./cmd/entrag ask "how to define order field in entgql"</summary>
 To define an order field in  entgql , you need to annotate a field in your  Ent
  schema using the  entgql.Annotation  with the  OrderField . Here's a step-by-
  step
  guide on how you can do it:

  1. Choose a Comparable Field: Select the field in your schema that you want to
  be orderable. This could be a text field, a timestamp, an integer, an enum, etc.
  2. Annotate the Field: Use the  entgql.OrderField  annotation on the chosen
  field. The order field name should be uppercase and match its corresponding enum
  value in the GraphQL schema.
  3. Update your Schema: Here’s an example of how to annotate your fields in the
  ent/schema :
    ```go
    func (Todo) Fields() []ent.Field {
        return []ent.Field{
            field.Text("text").
                NotEmpty().
                Annotations(
                    entgql.OrderField("TEXT"),
                ),
            field.Time("created_at").
                Default(time.Now).
                Immutable().
                Annotations(
                    entgql.OrderField("CREATED_AT"),
                ),
            field.Enum("status").
                NamedValues(
                    "InProgress", "IN_PROGRESS",
                    "Completed", "COMPLETED",
                ).
                Default("IN_PROGRESS").
                Annotations(
                    entgql.OrderField("STATUS"),
                ),
            field.Int("priority").
                Default(0).
                Annotations(
                    entgql.OrderField("PRIORITY"),
                ),
        }
    }
    ```
  4. Alternatively, Multiple Order Fields: If you want to enable ordering by
  multiple fields, you can use the  entgql.MultiOrder()  annotation:
    ```go
    func (Todo) Annotations() []schema.Annotation {
        return []schema.Annotation{
            entgql.MultiOrder(),
        }
    }
    ```

  5. Generate GraphQL Types: Ensure that the GraphQL types match this setup in
  your GraphQL schema, for example:
    ```graphql
    enum OrderDirection {
      ASC
      DESC
    }
    enum TodoOrderField {
      CREATED_AT
      PRIORITY
      STATUS
      TEXT
    }
    input TodoOrder {
      direction: OrderDirection!
      field: TodoOrderField
    }
    ```

  6. Add  orderBy  Argument to Queries: Include the  orderBy  parameter in your
  GraphQL queries to allow clients to sort items:
    ```graphql
    type Query {
      todos(
        after: Cursor
        first: Int
        before: Cursor
        last: Int
        orderBy: TodoOrder
      ): TodoConnection!
    }
    ```

  By following these steps, you ensure that your Ent-based application can support
  sorting in your GraphQL API based on various fields.


</details>

<details>
<summary> go run ./cmd/entrag ask "what's the difference between privacy rules and interceptors" </summary>
 Privacy rules and interceptors serve different purposes within the Ent
  framework, each providing distinct functionalities:

1. Privacy Rules:
* Purpose: Privacy rules are primarily concerned with enforcing access
control policies for data queries and mutations. They determine whether an
operation is allowed or denied based on predefined rules.
* Implementation: They are implemented as part of a privacy policy using the
ent.Policy  interface which includes  EvalQuery  and  EvalMutation  methods.
* Operation: Privacy rules evaluate whether specific conditions are met
before allowing or denying access to the data. They can return decisions
like  privacy.Allow ,  privacy.Deny , or  privacy.Skip  to control the flow
of evaluation.
* Use Case: Ideal for managing access control by ensuring that users can
only perform certain operations if they meet the specified criteria.
2. Interceptors:
* Purpose: Interceptors act as middleware for Ent queries, allowing
modification and customization of query behaviors. They can be used to
augment or modify queries during different stages of their lifecycle.
* Implementation: Implemented as interfaces or using the  ent.InterceptFunc
adapter. They intercept and potentially modify queries by working on the
read-path.
* Operation: Interceptors modify or enhance queries, typically without the
access control logic inherent in privacy rules. They provide hooks to
execute custom logic pre and post query execution.
* Use Case: Suitable for generic transformations or modifications to queries,
such as adding default filters, query limitations, or logging operations
without focusing on access control.


In summary, while privacy rules focus on access control, interceptors are about
managing and modifying the query execution process.
</details>

### 总结

本文中，我们探索了如何用Ent、Atlas和pgvector构建RAG系统。特别感谢[Eli Bendersky](https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/)的精彩博文，以及他多年来在Go语言领域的卓越写作！